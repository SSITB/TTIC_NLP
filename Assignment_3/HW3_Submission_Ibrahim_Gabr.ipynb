{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ibrahim Gabr - 12144496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore',category=FutureWarning)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "%run preprocessing.py # pre-processing function, uses helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1\n",
    "\n",
    "Here are my Sentences:\n",
    "\n",
    "< s > Ibrahim eats < /s>\n",
    "\n",
    "< s > Ibrahim < /s>\n",
    "\n",
    "< s > Ibrahim < /s>\n",
    "\n",
    "< s > Ibrahim < /s>\n",
    "\n",
    "< s > Ibrahim < /s>\n",
    "\n",
    "< s > Ibrahim < /s>\n",
    "\n",
    "**Model U:**\n",
    "\n",
    "**Bigram, count, probability:**\n",
    "\n",
    "< s > Ibrahim, 6, 0.5\n",
    "\n",
    "Ibrahim < /s>, 5, 0.417\n",
    "\n",
    "< s > eats, 0, 0\n",
    "\n",
    "eats < /s>, 1, 0.083\n",
    "\n",
    "**Model S:**\n",
    "\n",
    "**Bigram, count, probability:**\n",
    "\n",
    "< s > Ibrahim, 7, 0.4375\n",
    "\n",
    "Ibrahim < /s>, 6, 0.375\n",
    "\n",
    "< s > eats, 1, 0.0625\n",
    "\n",
    "eats < /s>, 2, 0.125\n",
    "\n",
    "**Results:**\n",
    "\n",
    "**sentence: < s > Ibrahim eats < /s>**\n",
    "\n",
    "Under Model U, the probability of this sentence is: $0.5 \\cdot 0.083 = 0.0415$\n",
    "\n",
    "Under Model S, the probability of this sentence is: $0.4375 \\cdot 0.125 = 0.0546875$\n",
    "\n",
    "As such, this sentence has a **higher probability under Model S than Model U.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tweets = open(\"tweets-train.txt\", \"r\").readlines()\n",
    "devtest_tweets = open(\"tweets-devtest.txt\", \"r\").readlines()\n",
    "dev_tweets = open(\"tweets-dev.txt\", \"r\").readlines()\n",
    "embeddings = open(\"embeddings-twitter.txt\", \"r\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input, training_target = pre_process_file(training_tweets, training_tweets, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_input, dev_target = pre_process_file(training_tweets, dev_tweets, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_test_input, dev_test_target = pre_process_file(training_tweets, devtest_tweets, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2.1\n",
    "\n",
    "Accuracy on Dev Test is **85.6%**. Code is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14619 samples, validate on 4823 samples\n",
      "Epoch 1/100\n",
      "14619/14619 [==============================] - 1s 56us/step - loss: 1.3758 - acc: 0.6585 - val_loss: 0.7945 - val_acc: 0.7871\n",
      "Epoch 2/100\n",
      "14619/14619 [==============================] - 1s 44us/step - loss: 0.6481 - acc: 0.8206 - val_loss: 0.6031 - val_acc: 0.8350\n",
      "Epoch 3/100\n",
      "14619/14619 [==============================] - 1s 45us/step - loss: 0.5356 - acc: 0.8489 - val_loss: 0.5483 - val_acc: 0.8453\n",
      "Epoch 4/100\n",
      "14619/14619 [==============================] - 1s 46us/step - loss: 0.4866 - acc: 0.8607 - val_loss: 0.5241 - val_acc: 0.8488\n",
      "Epoch 5/100\n",
      "14619/14619 [==============================] - 1s 51us/step - loss: 0.4588 - acc: 0.8676 - val_loss: 0.5134 - val_acc: 0.8503\n",
      "Epoch 6/100\n",
      "14619/14619 [==============================] - 1s 46us/step - loss: 0.4393 - acc: 0.8719 - val_loss: 0.5060 - val_acc: 0.8530\n",
      "Epoch 7/100\n",
      "14619/14619 [==============================] - 1s 45us/step - loss: 0.4249 - acc: 0.8743 - val_loss: 0.5010 - val_acc: 0.8536\n",
      "Epoch 8/100\n",
      "14619/14619 [==============================] - 1s 46us/step - loss: 0.4134 - acc: 0.8787 - val_loss: 0.4957 - val_acc: 0.8551\n",
      "Epoch 9/100\n",
      "14619/14619 [==============================] - 1s 46us/step - loss: 0.4020 - acc: 0.8811 - val_loss: 0.4957 - val_acc: 0.8553\n",
      "Epoch 10/100\n",
      "14619/14619 [==============================] - 1s 46us/step - loss: 0.3926 - acc: 0.8839 - val_loss: 0.4919 - val_acc: 0.8588\n",
      "Epoch 11/100\n",
      "14619/14619 [==============================] - 1s 46us/step - loss: 0.3845 - acc: 0.8844 - val_loss: 0.4931 - val_acc: 0.8559\n",
      "Epoch 12/100\n",
      "14619/14619 [==============================] - 1s 45us/step - loss: 0.3782 - acc: 0.8881 - val_loss: 0.4920 - val_acc: 0.8578\n",
      "Epoch 13/100\n",
      "14619/14619 [==============================] - 1s 46us/step - loss: 0.3698 - acc: 0.8890 - val_loss: 0.4941 - val_acc: 0.8565\n",
      "Epoch 14/100\n",
      "14619/14619 [==============================] - 1s 43us/step - loss: 0.3632 - acc: 0.8906 - val_loss: 0.4946 - val_acc: 0.8590\n",
      "Epoch 15/100\n",
      "14619/14619 [==============================] - 1s 47us/step - loss: 0.3559 - acc: 0.8932 - val_loss: 0.4948 - val_acc: 0.8571\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(1,150), activation=\"tanh\")) # issue is here.\n",
    "model.add(Dense(25, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer=\"adam\",#using adam optimizer\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='best.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "history = model.fit(training_input,\n",
    "                      training_target,\n",
    "                      epochs=100,\n",
    "                      callbacks=callbacks,\n",
    "                      batch_size=50,\n",
    "                      validation_data=(dev_input, dev_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model(\"best.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7152/7152 [==============================] - 0s 14us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4807981384347216, 0.8561241612071693]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.evaluate(dev_test_input, dev_test_target, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2\n",
    "\n",
    "In this section, I add 3 layers, all with the relu activation function in addition to 50% dropout between every layer.\n",
    "\n",
    "This results in an accuracy of ~ 87.4%.\n",
    "\n",
    "This is around a 2% increase from the baseline architecture.\n",
    "\n",
    "The literature suggests that relu is the best acivation function to use, this is why I chose it as the activation.\n",
    "\n",
    "Furthermore, while early stopping is itself a form of regularization, adding dropout to \"further\" regularize the model seems to have improved performance.\n",
    "\n",
    "**Experimentation Details:**\n",
    "\n",
    "I experimented with higher dropout (0.7) and adding a fourth layer of 4096 nodes, and this did not imporve network performance.\n",
    "\n",
    "I also experimented with changing the activation used in different layers, for example: Relu -> tanh -> relu -> sigmoid. This did not result in any performance gains.\n",
    "\n",
    "I tried to research whether there are any benefits of altering the activation fucntion on a layer by layer basis and could find no conclusive evidence that it does. As such, each layer just employs relu as the activation function.\n",
    "\n",
    "The adam optimizer outpeformed SGD. Furthermore, my early stopping criterion was 2, that is, after seeing no improvement in validation loss for 2 epochs, stop training the network. Initially, I set this to 5 in the baseline model, however, reducing the `patience` led to better model performance.\n",
    "\n",
    "Lastly, there does seem to be some tradeoff between the **depth** and the **width** of the model. I have found that having a wider model (i.e more nodes in a given layer) gives better performance off the bat, whereas having a deeping network could lead to marginal performance gains (i.e. Accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14619 samples, validate on 4823 samples\n",
      "Epoch 1/100\n",
      "14619/14619 [==============================] - 16s 1ms/step - loss: 1.0919 - acc: 0.6856 - val_loss: 0.5893 - val_acc: 0.8325\n",
      "Epoch 2/100\n",
      "14619/14619 [==============================] - 15s 998us/step - loss: 0.6083 - acc: 0.8195 - val_loss: 0.5121 - val_acc: 0.8553\n",
      "Epoch 3/100\n",
      "14619/14619 [==============================] - 14s 982us/step - loss: 0.5104 - acc: 0.8466 - val_loss: 0.4858 - val_acc: 0.8627\n",
      "Epoch 4/100\n",
      "14619/14619 [==============================] - 15s 995us/step - loss: 0.4604 - acc: 0.8626 - val_loss: 0.4742 - val_acc: 0.8642\n",
      "Epoch 5/100\n",
      "14619/14619 [==============================] - 14s 946us/step - loss: 0.4166 - acc: 0.8753 - val_loss: 0.4655 - val_acc: 0.8748\n",
      "Epoch 6/100\n",
      "14619/14619 [==============================] - 15s 1ms/step - loss: 0.3875 - acc: 0.8808 - val_loss: 0.4615 - val_acc: 0.8754\n",
      "Epoch 7/100\n",
      "14619/14619 [==============================] - 15s 1ms/step - loss: 0.3641 - acc: 0.8884 - val_loss: 0.4481 - val_acc: 0.8766\n",
      "Epoch 8/100\n",
      "14619/14619 [==============================] - 15s 1ms/step - loss: 0.3444 - acc: 0.8954 - val_loss: 0.4631 - val_acc: 0.8744\n",
      "Epoch 9/100\n",
      "14619/14619 [==============================] - 15s 1ms/step - loss: 0.3276 - acc: 0.8984 - val_loss: 0.4643 - val_acc: 0.8750\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(1,150), activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2048, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(25, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', # Cross-entropy\n",
    "                optimizer=\"adam\",#using adam optimizer\n",
    "                metrics=['accuracy']) # Accuracy performance metric\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=2),\n",
    "             ModelCheckpoint(filepath='best_2_layers.h5', monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "history = model.fit(training_input,\n",
    "                      training_target,\n",
    "                      epochs=100,\n",
    "                      callbacks=callbacks,\n",
    "                      batch_size=50,\n",
    "                      validation_data=(dev_input, dev_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7152/7152 [==============================] - 1s 171us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.435658347042865, 0.8740212483460738]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model = load_model(\"best_2_layers.h5\")\n",
    "best_model.evaluate(dev_test_input, dev_test_target, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
